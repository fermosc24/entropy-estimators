
# Entropy Estimators

A Python library for computing entropy and divergence estimates from discrete data using various statistical estimators. Includes advanced estimators like James-Stein shrinkage, Chao-Shen, and Nemenman-Shafee-Bialek (NSB), as well as divergence metrics like KL and Jensen-Shannon.

---

## ğŸ“¦ Installation

Install locally in development mode:

```bash
git clone https://github.com/fermosc24/entropy-estimators.git
cd entropy-estimators
pip install -e .
```

Dependencies:
- `numpy`
- `scipy`
- `mpmath`

---

## ğŸ§  Features

- **Entropy estimation methods:**
  - Maximum Likelihood (MLE)
  - James-Stein Shrinkage Estimator (JSE)
  - Chao-Shen (CAE)
  - Chao-Wang-Jost (CWJ)
  - Nemenman-Bialek-de Ruyter van Steveninck (NBRS)
  - Nemenman-Shafee-Bialek (NSB)

- **Divergence metrics:**
  - KL Divergence (James-Stein smoothed)
  - Jensen-Shannon Divergence

- **Utility tools:**
  - `FreqShrink`: James-Stein frequency smoothing
  - `sample_frequencies`: Sampling from discrete frequency arrays
  - `dict_to_ndarray`: Convert labeled dictionary to tensor + label maps

---

## ğŸ”§ Usage

### Entropy estimation

```python
from entropy_estimators import Entropy

counts = [5, 3, 2, 0, 1]
entropy = Entropy(counts, method="NSB")
print("Entropy:", entropy)
```

### Jensen-Shannon divergence

```python
from entropy_estimators import JS_JensenShannon

a = [10, 0, 5, 2]
b = [5, 3, 5, 4]

jsd = JS_JensenShannon(a, b)
print("Jensen-Shannon divergence:", jsd)
```

---

## ğŸ“ Project Structure

```
entropy-estimators/
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ src/
â”‚   â””â”€â”€ entropy_estimators/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ core.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_entropy_estimators.py
```

---

## ğŸ“œ License

MIT License Â© 2025 [fermosc24](https://github.com/fermosc24)
